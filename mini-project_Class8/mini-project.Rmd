---
title: 'Class 8 Mini-Project: Unsupervised Learning'
author: "Taylor F. (A59010460)"
date: "2/11/2022"
output:
  pdf_document: default
  html_document: default
---

# Importing the Data

Here we analyze data from the University of Wisconsin Medical Center on Breast Cancer FNA.

```{r}
#Step 1: Download the Data Set

#Step 2: Place the file in the project folder

# Step 3: Complete the following code to input the data and store as wisc.df
wisc.df <- read.csv("WisconsinCancer.csv", row.names=1)

# Step 4 (optional): Check the file
head(wisc.df)
```

The first column, the diagnosis, is not necessary for our analysis. With that in mind, we will make a new data frame that omits this column.

```{r}
# We can use -1 here to remove the first column
wisc.data <- wisc.df[,-1]
```


Question 1: How many observations are in this dataset?

```{r}
nrow(wisc.data)
```

Question 2: How many of the observations have a malignant diagnosis (i.e. how many Ms and Bs are there)?

```{r}
#Number of malignant diagnoses
sum(wisc.df$diagnosis == "M")

#Number of benign diagnoses
sum(wisc.df$diagnosis == "B")
```

Is there another way, easier, way to answer Question 2?

```{r}
#Create a table of the relevant metrics
table(wisc.df$diagnosis)
```

Question 3: How many variables/features in the data are suffixed with _mean?

```{r}
#We can use the grep() function, that searches a given data frame for a specified term, in this case the column names (stored as colnames())

#We can then use the length() function to arrive at a total
length(grep("_mean", colnames(wisc.df)))
```

It will also be helpful to create a "diagnosis" variable for later, made from the diagnosis column of the "wisc.df" data frame. We can store it a factor (using as.factor()) and use it to plot with later.

```{r}
# Create diagnosis factor for later 
diagnosis <- as.factor(wisc.df$diagnosis)
diagnosis
```

# Principle Component Analysis

The main funciton in base R for PCA is "prcomp()". There is an important optional argument called "scale" in this function.

Before we scale, we should check the data to determine if this step is necessary


```{r}
# Check column means and standard deviations
colMeans(wisc.data)

apply(wisc.data,2,sd)
```

The first line does the principle component analysis (while scaling the data), the second line shows a summary 

```{r}
wisc.pr <- prcomp(wisc.data, scale = TRUE)
summary(wisc.pr)
```

Question 4: From your results, what proportion of the original variance is captured by the first principal components (PC1)?

44.27%

Question 5: How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

3 PCs

Question 6: How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

7 PCs

The main result of these types of methods is called a PCA plot (a.k.a. score plot. ordination plot)

Question 7: Make a biplot of the PC data

```{r}
biplot(wisc.pr)
#This is a really bad, messy plot
```

A summary of the "x" for wisc.pr

```{r}
#wisc.pr$x
```

We will now plot PC1 vs. PC2 x values

```{r}
#We can use a simple plot to see how the two compare
plot(wisc.pr$x[,1:2], col = diagnosis)
```

Question 8: Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

```{r}
plot(wisc.pr$x[,c(1,3)], col = diagnosis)
```

This graph has greater overlap between the two groups, as less of the variance can be explained by PC1 + PC3 than PC1 + PC2.

```{r}
#Rotations of the data for PC1
wisc.pr$rotation[,1]
```

Question 9: For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?

-0.26085376

Question 10: What is the minimum number of principal components required to explain 80% of the variance of the data?

5 PCs

These plots describe the percentage of variance that can be explained by each PC

```{r}
# Calculate variance of each component
pr.var <- wisc.pr$sdev^2
head(pr.var)

# Variance explained by each principal component: pve
pve <- pr.var / sum(pr.var)
  
# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")

# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )

#We can create an even more descriptive graph
#library(ggplot2)
#install.packages("factoextra")
#library(factoextra)
#fviz_eig(wisc.pr, addlabels = TRUE)
```


## Hierarchical clustering of raw data is not very helpful

```{r}
#Using the minimum number of principal components required to describe at least 90% of the variability in the data (i.e. 7 PCs), we can create a hierarchical clustering model with the linkage method = "ward.D2". We use Wardâ€™s criterion here because it is based on multidimensional variance like principal components analysis. 
wisc.pr.hclust <- hclust(dist(wisc.pr$x[,1:7]), method = "ward.D2")
plot(wisc.pr.hclust)
```


It looks as though the data divides into two groups, which could map onto the "benign" and "malignant" categories. Let's find out. Using the cutree() function, we can sort the clusters into two membership groups.

```{r}
#We can first ask how many data points are in each of the two groups
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)

#We can plot PC1 vs. PC2 again, coloring by grps to see how it compares to coloring by diagnosis
plot(wisc.pr$x[,1:2], col = grps)


#Next, we can use our diagnosis factor to see how many of each diagnosis are in each of these two groups
table(grps, diagnosis)
```










